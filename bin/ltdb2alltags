#!/usr/bin/python2

import os, sys, csv, json
from argparse import ArgumentParser
import xml.etree.cElementTree as et
import palaso.langtags as lt
import csv, urllib2, codecs

def find_file(tagstr, root='.') :
    fname = tagstr.replace('-', '_') + '.xml'
    testf = os.path.join(root, fname)
    if os.path.exists(testf) : return testf
    testf = os.path.join(root, fname[0], fname)
    if os.path.exists(testf) : return testf
    return None

def issimple(testf, getname=None):
    try:
        doc = et.parse(testf)
    except:
        return (False, "")
    if len(doc.getroot()) > 1 :
        if getname is not None:
            e = doc.find('./localeDisplayNames/languages/language[@type="{}"]'.format(getname))
            if e is not None:
                return (False, unicode(e.text))
        return (False, "")
    return (True, "")

def process_cldrentry(s, lts, allentries, root='.'):
    s = s.replace("_", "-")
    if s in lts:
        return
    l = lt.LangTag(s)
    if l.variants is not None and 'posix' in l.variants:
        return
    if l.region is not None:
        t = lt.LangTag(tag=None, lang=l.lang, script=l.script, variants=l.variants, extensions=l.extensions)
        n = None
        if t in lts:
            n = lts[t]
        elif str(t) in allentries:
            if str(l.region) in allentries[str(t)][1]:
                n = allentries[str(t)][0]
        if n is not None:
            if l.script is None:
                l.script = n.script
                l.hidescript = True
        testf = find_file(str(t), root=root)
        if testf is not None:
            l.parent = t
            l.parentsame = issimple(find_file(s, root))[0]
    lts.add(l)
    return l

def addnames(n, b, alllangrefs, names, regions):
    regionlist = set([b.region] + getattr(b, 'regions', []))
    for reg in regionlist:
        if reg is not None:
            regions.add(reg)
        lname = "{}-{}".format(getattr(b, 'iso639', b.lang), reg)
        alllangreg.add(lname)
        if lname in names:
            if 'name' in n:
                n['names'].append(names[lname][0][0])
            else:
                n['name'] = names[lname][0][0]
            n['names'].extend(names[lname][1])

def get_regionnames():
    res = {}
    fname = os.path.join(os.path.dirname(lt.__file__), "sldr", "language-subtag-registry.txt")
    with open(fname) as f :
        currlang = None
        mode = None
        deprecated = False
        for l in f.readlines() :
            l = l.strip()
            if l.startswith("Type: ") :
                mode = l[6:]
                if currlang is not None and currname is not None:
                    res[currlang] = currname
                currlang = None
                currname = None
            elif l.startswith("Subtag: ") :
                if mode == "region":
                    currlang = l[8:]
            elif l.startswith("Description: "):
                if mode == "region":
                    currname = l[13:]
        if currlang is not None:
            res[currlang] = currname
    return res

parser = ArgumentParser()
parser.add_argument('infile', help='Input CSV file from Google drive')
parser.add_argument('outfile', help='alltags.txt output file')
parser.add_argument('-i','--indir', default='.', help='Directory containing sldr tree')
parser.add_argument('-p','--operators',action="store_true",default=False,help="Output complex operators")
parser.add_argument('-t','--text',action="store_true",default=False,help="Output text format")
parser.add_argument('-L','--langindex',help="Use local LanguageInfo.tab")
args = parser.parse_args()

lts = lt.LangTags(noalltags=True)
deprecated = set()
iananames = {}
for k,v in lts.items():
    if getattr(v, 'deprecated', False):
        deprecated.add(v.lang)
    if hasattr(v, 'desc'):
        iananames[k] = v.desc
regionnames = get_regionnames()
lts.clear()
allentries = {}
allmacros = {}
with open(args.infile) as csvfile :
    reader = csv.DictReader(csvfile)
    for row in reader :
#        if 'CLDR' in row['confirmed'] : continue
        lid = row['Lang_Id']
        ls = row['likely_subtag']
        i6 = row['ISO 639-3']
        d = row['deprecated']
        macro = row['Macro']
        t = lt.LangTag(ls)
        b = lt.LangTag(lid)
        if b.lang == "000":
            continue
        if d != "":
            deprecated.add(t.lang)
        if b.script in [None, 'Zyyy', 'Qaax']:
            t.hidescript = True
        if b.region is None :
            t.hideregion = True
        t.hideboth |= (t.hideregion and t.hidescript)
        if t.lang in deprecated:
            t.deprecated = True
        t.regions = row['regions'].split()
        t.name = row['LangName']
        lts.add(t)
        if repr(t) in lts:
            lts[repr(t)].iso639 = i6
        allentries[lid] = (t, row['regions'].split(' '))
        if macro != "":
            s = str(t)
            m = lt.LangTag(macro)
            m.merge_equivalent(t)
            allmacros[s] = repr(m)

for l in os.listdir(args.indir) :
    if l.endswith('.xml') :
        if 1 < len(l.split('_', 1)[0]) < 4 :
            process_cldrentry(l[:-4], lts, allentries, root=args.indir)
    elif len(l) == 1 and os.path.isdir(os.path.join(args.indir, l)) :
        for s in os.listdir(os.path.join(args.indir, l)) :
            if s.endswith('.xml') :
                if 1 < len(s.split('_', 1)[0]) < 4 :
                    process_cldrentry(s[:-4], lts, allentries, root=args.indir)

for k, v in allmacros.items():
    try:
        base = lts[k]
        macro = lts[v]
    except KeyError:
        macro = lt.LangTag(v)
        lts.add(macro)
        if base.script is None:
            macro.hidescript = True
        if base.region is None:
            macro.hideregion = True
    base.skip = True
    if not hasattr(macro, 'regions'):
        macro.regions = []
    if hasattr(base, 'regions'):
        macro.regions.extend(base.regions)
    macro.base.append(base)

    
if args.text:
    with open(args.outfile, "w") as alltags :
        res = lts.generate_alltags()
        
        outstrings = []
        for line in res:
            lineres = [[" = ", x] for x in line if x]
            if not len(lineres):
                continue
            lineres[0][0] = ""
            hasstar = False
            for l in lineres:
                tf = find_file(l[1], root=args.indir)
                if tf is not None:
                    if hasstar:
                        if args.operators:
                            l[0] = " |= " if issimple(tf)[0] else " <= "
    #                    else:
    #                        l[0] = " | "
                    elif args.operators and hasattr(lts.get(l[1], l[1]), "parent"):
                        x = lts[l[1]]
                        l[1] = l[1] + (" >| " if x.parentsame else " > ") + "*" + str(x.parent)
                    l[1] = "*" + l[1]
                    hasstar = True
            outstrings.append("".join(x[0] + x[1] for x in lineres))
        alltags.write("\n".join(sorted(outstrings, key=lambda x:x.replace('*', ''))) + "\n")
else:
    names = {}
    if args.langindex:
        namef = open(args.langindex, "r")
    else:
        req = urllib2.Request(url="https://www.ethnologue.com/codes/LanguageIndex.tab",
                              headers = {"User-Agent": "Mozilla"})
        namef = urllib2.urlopen(req)
    reader = csv.DictReader(namef, dialect=csv.excel_tab)
    for r in reader:
        if r['CountryID'] == 'ET' and r['NameType'] == 'LA':     # hack fix for buggy Ethnologue data. To Be Reviewed 21/Feb/2019
            continue
        if r['NameType'].startswith("L") and r['CountryID'] and not r['NameType'].endswith("P"):
            names.setdefault(r['LangID']+"-"+r['CountryID'], [[],[]])[0 if r['NameType'] == 'L' else 1].append(r['Name'])
    namef.close()

    collisions = {}
    res = []
    alllangreg = set()
    allcomplexes = []
    for t in sorted(set(lts.values())):
        if t.skip:
            continue
        r = t.allforms()
        complexrs = []
        if not len(r[0]):
            continue
        n = {'tag': r[0], 'full': r[-1], 'region': t.region}
        if t.region in regionnames:
            n['regionname'] = regionnames[t.region]
        if len(r) > 2:
            n['tags'] = r[1:-1]
        hassldr = False
        for rf in r:
            tf = find_file(rf, root=args.indir)
            if tf is not None:
                hassldr = True
                (s, name) = issimple(tf, t.lang)
                if not s:
                    complexrs.append(rf)
                    if name is not None and name != "":
                        n['localname'] = name
        if len(complexrs) > 1:
            allcomplexes.append(complexrs)
        n['sldr'] = hassldr
        if hasattr(t, 'iso639'):
            n['iso639_3'] = t.iso639
        elif t.base is not None and len(t.base):
            for b in t.base:
                if hasattr(n, 'iso639'):
                    n['iso639_3'] = b.iso639
                    break
        elif t.lang in lts:
            x = lts[t.lang]
            if hasattr(x, 'iso639'):
                n['iso639_3'] = x.iso639
        n['names'] = []
        regions = set()
        addnames(n, t, alllangreg, names, regions)
        for b in t.base:
            addnames(n, b, alllangreg, names, regions)
        if t.lang in iananames:
            ns = iananames[t.lang]
            if 'name' not in n:
                n['name'] = ns[0]
                if len(ns) > 1 :
                    n['names'] = ns[1:]
            n['iana'] = ns if len(ns) > 1 else ns[0]
        if 'name' in n:
            n['names'] = sorted(x for x in set(n['names']) - set([n['name']]) if len(x))
        elif hasattr(t, 'name'):
            n['name'] = t.name
        else:
            print("Missing name entry for {} = {}".format(n['tag'], n['full']))
        if not len(n['names']):
            del n['names']
        if not len(n.get('iso639_3', "fred")):
            del n['iso639_3']
        if getattr(t, 'deprecated', False):
            n['deprecated'] = True
        if t.region is not None:
            regions.remove(t.region)
        if len(regions):
            n['regions'] = " ".join(sorted(regions))
        res.append(n)
        if n['tag'] in collisions:
            print("Multiple entries for: {} = {}, {}".format(n['tag'], n['full'], collisions[n['tag']]))
        else:
            collisions[n['tag']] = n['full']
        for k, v in n.items():
            if v is None:
                print("Null entry for {} in {}".format(k, n['tag']))
    with codecs.open(args.outfile, "w", encoding="utf-8") as fh:
        # use encoding="utf8" (not "utf-8") as a hack around a bug in json: https://stackoverflow.com/a/40777484/10488020
        json.dump(sorted(res, key=lambda x:x['tag']), fh, sort_keys=True, indent=4, ensure_ascii=False, encoding="utf8")
    missing = set(names.keys()) - alllangreg
    if len(missing):
        print("Missing language regions ({}): ".format(len(missing)) + " ".join(sorted(missing)))
    if len(allcomplexes):
        print("Complex relations between: " + ", ".join("[" + " ".join(x) + "]" for x in allcomplexes))



